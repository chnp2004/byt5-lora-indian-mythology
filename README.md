# byt5-lora-indian-mythology
This project fine-tunes a Byte-Level Sequence-to-Sequence (Seq2Seq) Language Model for Indian mythology using Low-Rank Adaptation (LoRA) and Knowledge Graph (KG)–enhanced training. Built on ByT5-small, the model learns question–answer relationships from the Ramayana, handling multilingual text at the byte level without tokenization.
